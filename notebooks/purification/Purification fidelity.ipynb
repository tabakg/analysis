{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## setup analysis\n",
    "execfile(r'D:\\measuring\\analysis\\scripts\\setup_analysis.py')\n",
    "from analysis.lib.purification import purify_pq as ppq; reload(ppq)\n",
    "from analysis.lib.purification import purify_analysis as pa; reload(pa)\n",
    "%matplotlib inline\n",
    "\n",
    "folder_lt3 = r'D:\\measuring\\data\\Purification_lt3_raw_data'\n",
    "folder_lt4 = r'D:\\measuring\\data\\Purification_lt4_raw_data'\n",
    "# addendum = r'\\Purify_XX_First_Attempts'\n",
    "# addendum = r'\\Purify_25percent_theta'\n",
    "#addendum = r'\\Purify_35percent_theta'\n",
    "addendum = r'\\Purify_15percent_theta'\n",
    "folder_lt3 = folder_lt3+addendum\n",
    "folder_lt4 = folder_lt4+addendum\n",
    "### ssro calibrations\n",
    "day = '20160716_' # for ssro calibration\n",
    "\n",
    "ssro_calib_lt3 = day+'091002'\n",
    "ssro_calib_lt4 = day+'113952'\n",
    "\n",
    "Purify_XX = pa.purify_analysis('purification_analysis',folder_lt3,folder_lt4,ssro_calib_lt3,ssro_calib_lt4)\n",
    "Purify_YY = pa.purify_analysis('purification_analysis',folder_lt3,folder_lt4,ssro_calib_lt3,ssro_calib_lt4)\n",
    "Purify_ZZ = pa.purify_analysis('purification_analysis',folder_lt3,folder_lt4,ssro_calib_lt3,ssro_calib_lt4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### find timestamps for all three experiments (for pi/6)\n",
    "# ZZ_days = ['20160808','20160807','20160722','20160721','20160720']\n",
    "# XX_days = ZZ_days\n",
    "# YY_days = ['20160808','20160807','20160722','20160724','20160725']\n",
    "# ZZ_days_new_timings, XX_days_new_timings, YY_days_new_timings = ['20160811'],['20160811'],['20160811']\n",
    "\n",
    "### for the pi/4 data\n",
    "# ZZ_days = ['20160714','20160715','20160716','20160718','20160719']\n",
    "# XX_days = ZZ_days\n",
    "# YY_days = ['20160727','20160726']\n",
    "# ZZ_days_new_timings, XX_days_new_timings, YY_days_new_timings = ['20160816'],['20160816'],['20160816']\n",
    "\n",
    "\n",
    "### for pi/5\n",
    "# ZZ_days_new_timings = ['20160818','20160819']; YY_days_new_timings = ZZ_days_new_timings; XX_days_new_timings = ZZ_days_new_timings\n",
    "# ZZ_days = []; YY_days = []; XX_days = []\n",
    "\n",
    "### for pi/8 \n",
    "ZZ_days_new_timings = ['20160820','20160821','20160822']; YY_days_new_timings = ZZ_days_new_timings; XX_days_new_timings = ZZ_days_new_timings\n",
    "ZZ_days = []; YY_days = []; XX_days = []\n",
    "\n",
    "all_lt3_ZZ , all_lt4_ZZ = [],[]\n",
    "all_lt3_XX , all_lt4_XX = [],[]\n",
    "all_lt3_YY , all_lt4_YY = [],[]\n",
    "\n",
    "offsets_XX,offsets_YY,offsets_ZZ = [],[],[] # Hold offset to compensate for timing change for new APD\n",
    "correction_time = 2772.5e3-2791.5e3\n",
    "offsets_ch1_XX,offsets_ch1_YY,offsets_ch1_ZZ = [],[],[]\n",
    "start_offset_ch1 = 1.8e3\n",
    "\n",
    "for d in ZZ_days:\n",
    "\n",
    "    tstamp_lt3,tstamp_lt4 = Purify_ZZ.tstamps_for_both_setups(d,contains = 'ZZ')\n",
    "    #,newest_tstamp = '110000') ### newest timestamp allows for only taking parts of a day.\n",
    "    all_lt3_ZZ.extend(tstamp_lt3)\n",
    "    all_lt4_ZZ.extend(tstamp_lt4)\n",
    "    offsets_ZZ.extend(np.zeros(np.shape(tstamp_lt3)))\n",
    "    offsets_ch1_ZZ.extend(np.zeros(np.shape(tstamp_lt4)))\n",
    "    \n",
    "for d in YY_days:\n",
    "\n",
    "    tstamp_lt3,tstamp_lt4 = Purify_YY.tstamps_for_both_setups(d,contains = 'YY')\n",
    "    all_lt3_YY.extend(tstamp_lt3)\n",
    "    all_lt4_YY.extend(tstamp_lt4)\n",
    "    offsets_YY.extend(np.zeros(np.shape(tstamp_lt3)))\n",
    "    offsets_ch1_YY.extend(np.zeros(np.shape(tstamp_lt4)))\n",
    "    \n",
    "for d in XX_days:\n",
    "\n",
    "    tstamp_lt3,tstamp_lt4 = Purify_XX.tstamps_for_both_setups(d,contains = 'XX')\n",
    "    all_lt3_XX.extend(tstamp_lt3)\n",
    "    all_lt4_XX.extend(tstamp_lt4)\n",
    "    offsets_XX.extend(np.zeros(np.shape(tstamp_lt3)))\n",
    "    offsets_ch1_XX.extend(np.zeros(np.shape(tstamp_lt4)))\n",
    "    \n",
    "for d in ZZ_days_new_timings:\n",
    "\n",
    "    tstamp_lt3,tstamp_lt4 = Purify_ZZ.tstamps_for_both_setups(d,contains = 'ZZ')\n",
    "    #,newest_tstamp = '110000') ### newest timestamp allows for only taking parts of a day.\n",
    "    all_lt3_ZZ.extend(tstamp_lt3)\n",
    "    all_lt4_ZZ.extend(tstamp_lt4)\n",
    "    offsets_ZZ.extend(np.zeros(np.shape(tstamp_lt3))+ correction_time)\n",
    "    offsets_ch1_ZZ.extend(np.zeros(np.shape(tstamp_lt4))+ start_offset_ch1)\n",
    "    \n",
    "for d in YY_days_new_timings:\n",
    "\n",
    "    tstamp_lt3,tstamp_lt4 = Purify_YY.tstamps_for_both_setups(d,contains = 'YY')\n",
    "    all_lt3_YY.extend(tstamp_lt3)\n",
    "    all_lt4_YY.extend(tstamp_lt4)\n",
    "    offsets_YY.extend(np.zeros(np.shape(tstamp_lt3))+ correction_time)\n",
    "    offsets_ch1_YY.extend(np.zeros(np.shape(tstamp_lt4))+ start_offset_ch1)\n",
    "    \n",
    "for d in XX_days_new_timings:\n",
    "\n",
    "    tstamp_lt3,tstamp_lt4 = Purify_XX.tstamps_for_both_setups(d,contains = 'XX')\n",
    "    all_lt3_XX.extend(tstamp_lt3)\n",
    "    all_lt4_XX.extend(tstamp_lt4)\n",
    "    offsets_XX.extend(np.zeros(np.shape(tstamp_lt3))+ correction_time)\n",
    "    offsets_ch1_XX.extend(np.zeros(np.shape(tstamp_lt4))+ start_offset_ch1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### load the data\n",
    "Purify_XX.load_raw_data(all_lt3_XX,all_lt4_XX)\n",
    "Purify_XX.correct_pq_times(offsets = offsets_XX, offsets_ch1 = offsets_ch1_XX)\n",
    "Purify_YY.load_raw_data(all_lt3_YY,all_lt4_YY)\n",
    "Purify_YY.correct_pq_times(offsets = offsets_YY, offsets_ch1 = offsets_ch1_YY)\n",
    "Purify_ZZ.load_raw_data(all_lt3_ZZ,all_lt4_ZZ)\n",
    "Purify_ZZ.correct_pq_times(offsets = offsets_ZZ, offsets_ch1 = offsets_ch1_ZZ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sweep_parameter(param_name,sweep_pts,apply_ROC, plot_all = False):\n",
    "    \n",
    "    ## get correlation probabilties\n",
    "    psi_m_XX,psi_p_XX,psi_m_XX_corrs,psi_p_XX_corrs = Purify_XX.sweep_filter_parameter_vs_correlations(param_name,sweep_pts,\n",
    "                                                                         apply_ROC = apply_ROC,do_plot= False)\n",
    "    psi_m_YY,psi_p_YY,psi_m_YY_corrs,psi_p_YY_corrs = Purify_YY.sweep_filter_parameter_vs_correlations(param_name,sweep_pts,\n",
    "                                                                         apply_ROC = apply_ROC,do_plot= False)\n",
    "    psi_m_ZZ,psi_p_ZZ,psi_m_ZZ_corrs,psi_p_ZZ_corrs = Purify_ZZ.sweep_filter_parameter_vs_correlations(param_name,sweep_pts,\n",
    "                                                                         apply_ROC = apply_ROC,do_plot= False)       \n",
    "    ## calculate fidelities\n",
    "    psi_m_F = (psi_m_XX[1] + psi_m_YY[1] + psi_m_ZZ[1]+1)/4.\n",
    "    psi_p_F = (psi_p_XX[1] + psi_p_YY[1] + psi_p_ZZ[1]+1)/4.\n",
    "    \n",
    "    print psi_m_YY[1]\n",
    "    print psi_m_ZZ[1]\n",
    "    \n",
    "    ## calciulate error bars\n",
    "    psi_m_F_u = np.sqrt(psi_m_XX[2]**2 + psi_m_YY[2]**2 + psi_m_ZZ[2]**2)/4.\n",
    "    psi_p_F_u = np.sqrt(psi_p_XX[2]**2 + psi_p_YY[2]**2 + psi_p_ZZ[2]**2)/4.\n",
    "    \n",
    "    # calculate ebits    \n",
    "    ebits_p, ebits_p_u = pa.calculate_ebits(psi_m_YY,psi_m_ZZ,psi_m_XX_corrs)\n",
    "    ebits_m, ebits_m_u = pa.calculate_ebits(psi_p_YY,psi_p_ZZ,psi_p_XX_corrs)\n",
    "    \n",
    "    psi_F = 0.5*(psi_m_F+psi_p_F)\n",
    "    psi_F_u = 0.5*np.sqrt(psi_m_F_u**2+psi_p_F_u**2)\n",
    "    \n",
    "    fig  = plt.figure()\n",
    "    ax = plt.subplot()\n",
    "    ax.set_xlabel(param_name)\n",
    "    ax.set_ylabel('Fidelity')\n",
    "    plt.errorbar(sweep_pts,psi_m_F,psi_m_F_u,label = '-')\n",
    "    plt.errorbar(sweep_pts,psi_p_F,psi_p_F_u,label = '+')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.close('all')\n",
    " \n",
    "    fig  = plt.figure()\n",
    "    ax = plt.subplot()\n",
    "    ax.set_xlabel(param_name)\n",
    "    ax.set_ylabel('Fidelity (avg)')\n",
    "    plt.errorbar(sweep_pts,psi_F,psi_F_u)\n",
    "    plt.show()\n",
    "    plt.close('all')\n",
    "    \n",
    "    fig  = plt.figure()\n",
    "    ax = plt.subplot()\n",
    "    ax.set_xlabel(param_name)\n",
    "    ax.set_ylabel('ebits')\n",
    "\n",
    "    plt.errorbar(sweep_pts,ebits_p,ebits_p_u,label = '+')\n",
    "    plt.errorbar(sweep_pts,ebits_m,ebits_m_u,label = '-')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.close('all')\n",
    "    \n",
    "    ### get the rate for any given data point\n",
    "    ### keep in mind that this includes ALL down time of the experiment. Some might argue that this is an unfair comparison\n",
    "    ### we therefore also include a diagram that uses the operation time of the sequence only and the number of times the sequence has run\n",
    "    ### tomography is excluded from this calculated rate\n",
    "    total_time = 0\n",
    "    total_time += Purify_XX.get_total_time() + Purify_YY.get_total_time() + Purify_ZZ.get_total_time()\n",
    "    total_counts = psi_m_XX[0]+psi_p_XX[0]+psi_m_YY[0]+psi_p_YY[0]+psi_m_ZZ[0]+psi_p_ZZ[0]\n",
    "    fig  = plt.figure()\n",
    "    ax = plt.subplot()\n",
    "    ax.set_xlabel(param_name)\n",
    "    ax.set_ylabel('Rate (Hz)')\n",
    "    plt.plot(sweep_pts,total_counts/total_time, label = 'incl. overhead')\n",
    "    \n",
    "    total_time = 0\n",
    "    total_time += Purify_XX.estimate_sequence_time() + Purify_YY.estimate_sequence_time() + Purify_ZZ.estimate_sequence_time()\n",
    "    \n",
    "    plt.plot(sweep_pts,total_counts/total_time, label = 'excl. overhead')\n",
    "    plt.legend(loc = 2)\n",
    "    plt.show()\n",
    "    plt.close('all')\n",
    "    \n",
    "    ## plot individual correlations\n",
    "    if plot_all:\n",
    "        fig  = plt.figure()\n",
    "        ax = plt.subplot()\n",
    "        ax.set_xlabel(param_name)\n",
    "        ax.set_ylabel('Expectation value')\n",
    "        plt.errorbar(sweep_pts,psi_m_XX[1],psi_m_XX[2],label = 'XX')\n",
    "        plt.errorbar(sweep_pts,psi_m_YY[1],psi_m_YY[2],label = 'YY')\n",
    "        plt.errorbar(sweep_pts,psi_m_ZZ[1],psi_m_ZZ[2],label = 'ZZ')\n",
    "        plt.title('Psi_minus correlations')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        plt.close('all')\n",
    "        \n",
    "        fig  = plt.figure()\n",
    "        ax = plt.subplot()\n",
    "        ax.set_xlabel(param_name)\n",
    "        ax.set_ylabel('Expectation value')\n",
    "        plt.errorbar(sweep_pts,psi_p_XX[1],psi_p_XX[2],label = 'XX')\n",
    "        plt.errorbar(sweep_pts,psi_p_YY[1],psi_p_YY[2],label = 'YY')\n",
    "        plt.errorbar(sweep_pts,psi_p_ZZ[1],psi_p_ZZ[2],label = 'ZZ')\n",
    "        plt.title('Psi_plus correlations')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sweep_parameter('bin_w2',np.arange(50,540,50),apply_ROC = True,plot_all = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sweep_parameter('st_start',np.linspace(-4e3,4e3,10)+2773.5e3,apply_ROC = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sweep_parameter('max_reps_w1',np.arange(100,1000,50),apply_ROC = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sweep_parameter('min_cr_lt4_after',np.arange(1,20,2),apply_ROC = True)\n",
    "sweep_parameter('min_cr_lt3_after',np.arange(1,20,2),apply_ROC = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sweep_parameter('max_dt',np.linspace(2.5e3,40e3,10),apply_ROC = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sweep_parameter('st_len',np.linspace(3e3,40e3,10),apply_ROC = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
