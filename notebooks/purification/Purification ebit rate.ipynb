{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## setup analysis\n",
    "execfile(r'D:\\measuring\\analysis\\scripts\\setup_analysis.py')\n",
    "from analysis.lib.purification import purify_pq as ppq; reload(ppq)\n",
    "from analysis.lib.purification import purify_analysis as pa; reload(pa)\n",
    "%matplotlib inline\n",
    "\n",
    "base_folder_lt3 = r'D:\\measuring\\data\\Purification_lt3_raw_data'\n",
    "base_folder_lt4 = r'D:\\measuring\\data\\Purification_lt4_raw_data'\n",
    "\n",
    "### ssro calibrations\n",
    "day = '20160716_' # for ssro calibration\n",
    "ssro_calib_lt3 = day+'091002'\n",
    "ssro_calib_lt4 = day+'113952'\n",
    "\n",
    "thetas = [np.pi/4,np.pi/5,np.pi/6,np.pi/8]\n",
    "addenda = [r'\\Purify_XX_First_Attempts',r'\\Purify_35percent_theta',r'\\Purify_25percent_theta',r'\\Purify_15percent_theta']\n",
    "\n",
    "outcome_bins = np.arange(50,540,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_ebit_rate(outcome_bins,apply_ROC = True):\n",
    "    \n",
    "    ## get correlation probabilties\n",
    "    psi_m_XX,psi_p_XX,psi_m_XX_corrs,psi_p_XX_corrs = Purify_XX.sweep_filter_parameter_vs_correlations('bin_w2',outcome_bins,\n",
    "                                                                         apply_ROC = apply_ROC,do_plot= False)\n",
    "    psi_m_YY,psi_p_YY,psi_m_YY_corrs,psi_p_YY_corrs = Purify_YY.sweep_filter_parameter_vs_correlations('bin_w2',outcome_bins,\n",
    "                                                                         apply_ROC = apply_ROC,do_plot= False)\n",
    "    psi_m_ZZ,psi_p_ZZ,psi_m_ZZ_corrs,psi_p_ZZ_corrs = Purify_ZZ.sweep_filter_parameter_vs_correlations('bin_w2',outcome_bins,\n",
    "                                                                         apply_ROC = apply_ROC,do_plot= False)       \n",
    "    ## calculate fidelities\n",
    "    psi_m_F = (psi_m_XX[1] + psi_m_YY[1] + psi_m_ZZ[1]+1)/4.\n",
    "    psi_p_F = (psi_p_XX[1] + psi_p_YY[1] + psi_p_ZZ[1]+1)/4.\n",
    "    \n",
    "    ## calciulate error bars\n",
    "    psi_m_F_u = np.sqrt(psi_m_XX[2]**2 + psi_m_YY[2]**2 + psi_m_ZZ[2]**2)/4.\n",
    "    psi_p_F_u = np.sqrt(psi_p_XX[2]**2 + psi_p_YY[2]**2 + psi_p_ZZ[2]**2)/4.\n",
    "    \n",
    "    # calculate ebits    \n",
    "    ebits_p, ebits_p_u = pa.calculate_ebits(psi_m_YY,psi_m_ZZ,psi_m_XX_corrs)\n",
    "    ebits_m, ebits_m_u = pa.calculate_ebits(psi_p_YY,psi_p_ZZ,psi_p_XX_corrs)\n",
    "    \n",
    "    total_counts_p = psi_p_XX[0]+psi_p_YY[0]+psi_p_ZZ[0]\n",
    "    total_counts_m = psi_m_XX[0]+psi_m_YY[0]+psi_m_ZZ[0]\n",
    "    \n",
    "    total_time = Purify_XX.estimate_sequence_time() + Purify_YY.estimate_sequence_time() + Purify_ZZ.estimate_sequence_time()\n",
    "    \n",
    "    rate_p = np.array(total_counts_p)/total_time\n",
    "    rate_m = np.array(total_counts_m)/total_time\n",
    "    \n",
    "    ebit_rate_p = np.sum(rate_p*ebits_p)\n",
    "    ebit_rate_p_u = np.sqrt(np.sum(rate_p**2*ebits_p_u**2))\n",
    "    ebit_rate_m = np.sum(rate_m*ebits_m)\n",
    "    ebit_rate_m_u = np.sqrt(np.sum(rate_m**2*ebits_m_u**2))\n",
    "    \n",
    "    avg_fidelity_p = np.sum(psi_p_F*total_counts_p)/np.sum(total_counts_p)\n",
    "    avg_fidelity_p_u = np.sqrt(np.sum(psi_p_F_u**2*total_counts_p**2))/np.sum(total_counts_p)\n",
    "    avg_fidelity_m = np.sum(psi_m_F*total_counts_p)/np.sum(total_counts_m)\n",
    "    avg_fidelity_m_u = np.sqrt(np.sum(psi_m_F_u**2*total_counts_p**2))/np.sum(total_counts_m)\n",
    "    \n",
    "    rate_p = np.sum(rate_p)\n",
    "    rate_m = np.sum(rate_m)\n",
    "    \n",
    "    return [rate_p,rate_m,ebit_rate_p,ebit_rate_p_u,ebit_rate_m,ebit_rate_m_u,avg_fidelity_p,avg_fidelity_p_u,avg_fidelity_m,avg_fidelity_m_u]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vals = []\n",
    "\n",
    "for i,theta in enumerate(thetas):\n",
    "\n",
    "    folder_lt3 = base_folder_lt3+addenda[i]\n",
    "    folder_lt4 = base_folder_lt4+addenda[i]\n",
    "    \n",
    "    Purify_XX = pa.purify_analysis('purification_analysis',folder_lt3,folder_lt4,ssro_calib_lt3,ssro_calib_lt4)\n",
    "    Purify_YY = pa.purify_analysis('purification_analysis',folder_lt3,folder_lt4,ssro_calib_lt3,ssro_calib_lt4)\n",
    "    Purify_ZZ = pa.purify_analysis('purification_analysis',folder_lt3,folder_lt4,ssro_calib_lt3,ssro_calib_lt4)\n",
    "\n",
    "    if theta == np.pi/6:\n",
    "        ### find timestamps for all three experiments (for pi/6)\n",
    "        ZZ_days = ['20160808','20160807','20160722','20160721','20160720']\n",
    "        XX_days = ZZ_days\n",
    "        YY_days = ['20160808','20160807','20160722','20160724','20160725']\n",
    "        ZZ_days_new_timings, XX_days_new_timings, YY_days_new_timings = ['20160811'],['20160811'],['20160811']\n",
    "\n",
    "    elif theta == np.pi/4:\n",
    "        ZZ_days = ['20160714','20160715','20160716','20160718','20160719']\n",
    "        XX_days = ZZ_days\n",
    "        YY_days = ['20160727','20160726']\n",
    "        ZZ_days_new_timings, XX_days_new_timings, YY_days_new_timings = ['20160816'],['20160816'],['20160816']\n",
    "\n",
    "    elif theta == np.pi/5:\n",
    "        ZZ_days_new_timings = ['20160818','20160819']; YY_days_new_timings = ZZ_days_new_timings; XX_days_new_timings = ZZ_days_new_timings\n",
    "        ZZ_days = []; YY_days = []; XX_days = []\n",
    "\n",
    "\n",
    "    elif theta == np.pi/8:    \n",
    "        ZZ_days_new_timings = ['20160820','20160821','20160822']; YY_days_new_timings = ZZ_days_new_timings; XX_days_new_timings = ZZ_days_new_timings\n",
    "        ZZ_days = []; YY_days = []; XX_days = []\n",
    "\n",
    "    all_lt3_ZZ , all_lt4_ZZ = [],[]\n",
    "    all_lt3_XX , all_lt4_XX = [],[]\n",
    "    all_lt3_YY , all_lt4_YY = [],[]\n",
    "\n",
    "    offsets_XX,offsets_YY,offsets_ZZ = [],[],[] # Hold offset to compensate for timing change for new APD\n",
    "    correction_time = 2772.5e3-2791.5e3\n",
    "    offsets_ch1_XX,offsets_ch1_YY,offsets_ch1_ZZ = [],[],[]\n",
    "    start_offset_ch1 = 1.8e3\n",
    "\n",
    "    for d in ZZ_days:\n",
    "\n",
    "        tstamp_lt3,tstamp_lt4 = Purify_ZZ.tstamps_for_both_setups(d,contains = 'ZZ')\n",
    "        #,newest_tstamp = '110000') ### newest timestamp allows for only taking parts of a day.\n",
    "        all_lt3_ZZ.extend(tstamp_lt3)\n",
    "        all_lt4_ZZ.extend(tstamp_lt4)\n",
    "        offsets_ZZ.extend(np.zeros(np.shape(tstamp_lt3)))\n",
    "        offsets_ch1_ZZ.extend(np.zeros(np.shape(tstamp_lt4)))\n",
    "\n",
    "    for d in YY_days:\n",
    "\n",
    "        tstamp_lt3,tstamp_lt4 = Purify_YY.tstamps_for_both_setups(d,contains = 'YY')\n",
    "        all_lt3_YY.extend(tstamp_lt3)\n",
    "        all_lt4_YY.extend(tstamp_lt4)\n",
    "        offsets_YY.extend(np.zeros(np.shape(tstamp_lt3)))\n",
    "        offsets_ch1_YY.extend(np.zeros(np.shape(tstamp_lt4)))\n",
    "\n",
    "    for d in XX_days:\n",
    "\n",
    "        tstamp_lt3,tstamp_lt4 = Purify_XX.tstamps_for_both_setups(d,contains = 'XX')\n",
    "        all_lt3_XX.extend(tstamp_lt3)\n",
    "        all_lt4_XX.extend(tstamp_lt4)\n",
    "        offsets_XX.extend(np.zeros(np.shape(tstamp_lt3)))\n",
    "        offsets_ch1_XX.extend(np.zeros(np.shape(tstamp_lt4)))\n",
    "\n",
    "    for d in ZZ_days_new_timings:\n",
    "\n",
    "        tstamp_lt3,tstamp_lt4 = Purify_ZZ.tstamps_for_both_setups(d,contains = 'ZZ')\n",
    "        #,newest_tstamp = '110000') ### newest timestamp allows for only taking parts of a day.\n",
    "        all_lt3_ZZ.extend(tstamp_lt3)\n",
    "        all_lt4_ZZ.extend(tstamp_lt4)\n",
    "        offsets_ZZ.extend(np.zeros(np.shape(tstamp_lt3))+ correction_time)\n",
    "        offsets_ch1_ZZ.extend(np.zeros(np.shape(tstamp_lt4))+ start_offset_ch1)\n",
    "\n",
    "    for d in YY_days_new_timings:\n",
    "\n",
    "        tstamp_lt3,tstamp_lt4 = Purify_YY.tstamps_for_both_setups(d,contains = 'YY')\n",
    "        all_lt3_YY.extend(tstamp_lt3)\n",
    "        all_lt4_YY.extend(tstamp_lt4)\n",
    "        offsets_YY.extend(np.zeros(np.shape(tstamp_lt3))+ correction_time)\n",
    "        offsets_ch1_YY.extend(np.zeros(np.shape(tstamp_lt4))+ start_offset_ch1)\n",
    "\n",
    "    for d in XX_days_new_timings:\n",
    "\n",
    "        tstamp_lt3,tstamp_lt4 = Purify_XX.tstamps_for_both_setups(d,contains = 'XX')\n",
    "        all_lt3_XX.extend(tstamp_lt3)\n",
    "        all_lt4_XX.extend(tstamp_lt4)\n",
    "        offsets_XX.extend(np.zeros(np.shape(tstamp_lt3))+ correction_time)\n",
    "        offsets_ch1_XX.extend(np.zeros(np.shape(tstamp_lt4))+ start_offset_ch1) \n",
    "        \n",
    "    ### load the data\n",
    "    Purify_XX.load_raw_data(all_lt3_XX,all_lt4_XX)\n",
    "    Purify_XX.correct_pq_times(offsets = offsets_XX, offsets_ch1 = offsets_ch1_XX)\n",
    "    Purify_YY.load_raw_data(all_lt3_YY,all_lt4_YY)\n",
    "    Purify_YY.correct_pq_times(offsets = offsets_YY, offsets_ch1 = offsets_ch1_YY)\n",
    "    Purify_ZZ.load_raw_data(all_lt3_ZZ,all_lt4_ZZ)\n",
    "    Purify_ZZ.correct_pq_times(offsets = offsets_ZZ, offsets_ch1 = offsets_ch1_ZZ)\n",
    "    \n",
    "    vals.append(calculate_ebit_rate(outcome_bins = outcome_bins))\n",
    "   \n",
    "vals = np.transpose(vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rate_p = vals[0]\n",
    "rate_m = vals[1]\n",
    "ebit_rate_p = vals[2]\n",
    "ebit_rate_p_u = vals[3]\n",
    "ebit_rate_m = vals[4]\n",
    "ebit_rate_m_u = vals[5]\n",
    "avg_fidelity_p = vals[6]\n",
    "avg_fidelity_p_u = vals[7]\n",
    "avg_fidelity_m = vals[8]\n",
    "avg_fidelity_m_u = vals[9]\n",
    "\n",
    "super_posn_frac = np.sin(thetas)**2\n",
    "\n",
    "fig  = plt.figure()\n",
    "ax = plt.subplot()\n",
    "ax.set_xlabel('Superposn ms=0 frac.')\n",
    "ax.set_ylabel('Ebit rate')\n",
    "plt.errorbar(super_posn_frac,ebit_rate_m,ebit_rate_m_u,label = '-')\n",
    "plt.errorbar(super_posn_frac,ebit_rate_p,ebit_rate_p_u,label = '+')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.close('all')\n",
    "\n",
    "fig  = plt.figure()\n",
    "ax = plt.subplot()\n",
    "ax.set_xlabel('Superposn ms=0 frac.')\n",
    "ax.set_ylabel('Raw rate (Hz)')\n",
    "plt.plot(super_posn_frac,rate_m,label = '-')\n",
    "plt.plot(super_posn_frac,rate_p,label = '+')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.close('all')\n",
    "\n",
    "fig  = plt.figure()\n",
    "ax = plt.subplot()\n",
    "ax.set_xlabel('Superposn ms=0 frac.')\n",
    "ax.set_ylabel('Avg fidelity')\n",
    "plt.errorbar(super_posn_frac,avg_fidelity_m,avg_fidelity_m_u,label = '-')\n",
    "plt.errorbar(super_posn_frac,avg_fidelity_p,avg_fidelity_p_u,label = '+')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
